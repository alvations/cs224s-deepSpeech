Future Work
====

In the future, we will continue integrating more knowledge into
DeepSpeech, including high level features like coreferences,
dependency paths and knowledge base, and also integrate low-level
acoustic features.

We will further look into candidate generation with high-level
knowledge. For now we still treat the decoding problem as a
classification problem, which is picking from existing candidates.
However with linguistic, contextual and knowledge-base knowledge, we
might be able to generate new candidates into the lattice, and this
might enable us to beat oracle error rate.

It would be also interesting to ensemble different speech recognition systems to obtain larger lattices, and try to learn a model that aggregates different systems and enable inter-sistem corrections.


Conclusion
====

We present a model and a system that is able to integrate different levels of knowledge to decode word lattices generated by ASR systems. 

Our model is word-level conditional random fields, where word candidates are variables and features and domain-specific rules are factors. To obtain word-level training data in the factor graph, we distantly supervise the system by matching lattice words to transcripts, and label matched words on all best paths as true (others as false). Statistical learning and inference on these factor graphs give us rigorous probabilities that we can use to find best paths as outputs.

We present a system *DeepSpeech* that implements the above model. Our system is built on a scalable inference engine *DeepDive*, and is able to train and test on 400-hour speech data within 70 minutes.  
We propose to improve and release the system, with which developers will be able to add their own linguistic features easily, and conduct advanced decoding on their own datasets in real time.

We conduct experiments to evaluate DeepSpeech and explore different features. Evaluating with 150K broadcast news lattices, we get WER of 10.2% where the baseline Attila system gets WER up to 22.9%. 

In our study of different features, we have several insights including (1) word unigram and POS features does not help much; (2) a second confirmatory decoding system works better than a word-Ngram based language model; (3) DeepSpeech is able to utilize very sparse bag-of-Ngram features; (4) some CRF rules has different impact on WER and SER.

In the future we will release DeepSpeech system, integrate more high-level knowledge to approach oracle WER, study candidate generation methods to go over oracle WER, and try an ensemble system of different speech engines.


Acknowledgements
====

The authors would like to acknowledge Professor Christopher Re, for his support to this project and the DeepDive platform under rapid development.

The authors would also like to acknowledge Professor Dan Jurafsky for his encouragements and initial discussions regarding the project ideas.

Thanks to Andrew Mass for iterating the project ideas, giving hands-on suggestions and feedback.
Thanks to Ce Zhang for his inspiring ideas for candidate generation.
Thanks to Natalia Silveira and Stanford NLP group for providing us the datasets.
Thanks to Juergen Fesslmeier and Ana Maria Carrano from *Voyz.es*, for showing interest in the poster session and follow-up discussions.