Integrating Knowledge into Language Modeling in Speech Recognition with Probabilistic Models
====
Zifei Shan, Tianxin Zhao, and Haowen Cao
{zifei, tianxin, caohw}@stanford.edu

Speech recognition has been suffering from bad independence assumptions
across different phases. Acoustic models are trained without syntactic
and semantic knowledge beyond words, thus underestimated probabilities
of correct words are hard to be captured by subsequent naive language
models. We believe that eventually, joint inference on acoustic and
language models is an especially promising future. As a first step
towards this goal, we concentrate on advanced decoding in the language
model phase. We propose to integrate more knowledge in decoding the word
lattice, with the help of joint inference.

For state-of-the-art decoding approaches, it is hard to integrate
sophisticated knowledge because of scalability reasons. With the
cutting-edge researches on graph learning and sampling, we are now able
to perform massive learning with a speed of millions of variables per
second. Weaponed with this, we propose to use general factor graphs to
do learning and inference on word lattices, integrating different kinds
of knowledge including syntactic, semantic, context and higher-level
knowledge. If time permits, we will try generating new candidates
outside the lattice, with the help of knowledge.

Specifically, we aim to answer following questions:

1. How can different levels of knowledge help in speech recognition?
2. Can we approach or even beat the oracle error rate of word lattices?
3. How to make the trade-off of knowledge-featured language models 
   in real-time ASR applications within current technology?


Knowledge Taxonomy
----

Current state of the art in language models is using N-grams of words,
which is the simplest knowledge we can use. We propose a taxonomy of
knowledge that might help speech recognition:

- ASR-software specific knowledge: 
    - What errors that ASR systems tend to make?
    - Can we learn rules to fix software errors and generate 
      correct candidates?

- Corpus statistics: 
    - word-level: frequency of word N-grams in corpus
    - sentence level: frequency of sentences 
    - conversation level: frequency of conversations

- Deep linguistic features:
    - POS, NER and parsing

- High-level knowledge:
    - Topic-specific: weight words by topic
    - Speaker-specific: weight words by personal habit
    - Context-specific: condition on social relationships, emotion, 
      time, etc.

Experiments
----

There is no systematic experiments about how these different kinds of
knowledge can fix errors in speech recognition systems.  We propose to
conduct an error analysis about how different knowledge can improve
error rates in our system, and implement a most useful subset of
knowledge listed above.

We propose to experiment on several existing word lattices with
different lattice error rate ("oracle" error rate). We aim to generate
output sequences that can approach the oracle error rate, or even go
beyond it on bad-quality lattices (if time permits).


Related Works
----

We are conducting a relevant research in OCR systems. We treat OCR
softwares as black boxes, obtain their output sequences --- essentially
the same as a word lattice, and try to use knowledge to approach and
beat oracle error rate in the lattice.

In an error analysis, we have following interesting observations: 
(1) Most errors generated by OCR systems are automatically solvable 
    with a knowledge taxonomy similar to listed above.
(2) There is no one-fixes-all knowledge in OCR.
(3) The most useful kinds of knowledge are corpus statistics and 
    OCR-software specific knowledge.

With the insight from this relevant project, we propose to make use of
corpus statistics and ASR-software specific knowledge in our ASR system.

We will look into SCARF system and its applications meanwhile for comparison.

Dataset
----

We will be starting with the broadcast word lattice dataset by JHU
(LDC2011T06). We will continue looking for other word lattice datasets
with different oracle error rates.




=============================

Other thoughts:

- AM underestimates acoustic probability
- Beam search: cannot afford too wide beams.
    - wider -> more expensive
    - How about only searching in "valid" candidates?
    - Regenerating candidates later?

- generating smart candidates

- Google API, segmentation, rebuild by phones, supervised by ngrams